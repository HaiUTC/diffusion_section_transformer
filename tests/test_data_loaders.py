#!/usr/bin/env python3
"""
Test script for the data loader modules
Demonstrates how to use VisionLoader, StructureLoader, and LabelLoader
"""

import os
import json
from data_loaders import VisionLoader, StructureLoader, LabelLoader, DatasetLoader, validate_dataset

def test_individual_loaders():
    """Test each loader individually with the example data"""
    print("=== Testing Individual Data Loaders ===\n")
    
    # Test VisionLoader
    print("1. Testing VisionLoader...")
    vision_loader = VisionLoader(patch_size=16, target_size=512)
    
    try:
        vision_patches = vision_loader.load_and_process(
            "example-1/example.png", 
            width=1920, 
            height=824
        )
        print(f"   âœ“ Vision patches shape: {vision_patches.shape}")
        print(f"   âœ“ Patch dimensions: {vision_patches.shape[1]} (should be 16*16*3 = 768)")
        print(f"   âœ“ Number of patches: {vision_patches.shape[0]}")
    except Exception as e:
        print(f"   âœ— Vision loader failed: {e}")
    
    # Test StructureLoader
    print("\n2. Testing StructureLoader...")
    structure_loader = StructureLoader()
    
    try:
        # Load example structure data
        with open("example-1/example.json", 'r') as f:
            example_data = json.load(f)
        
        structure_tokens, hierarchy_embeddings = structure_loader.load_and_process(
            example_data['structure']['data']
        )
        print(f"   âœ“ Structure tokens shape: {structure_tokens.shape}")
        print(f"   âœ“ Hierarchy embeddings shape: {hierarchy_embeddings.shape}")
        print(f"   âœ“ Number of tokens: {len(structure_tokens)}")
        print(f"   âœ“ Vocabulary size: {len(structure_loader.token_to_id)}")
        
        # Show first few tokens
        print(f"   âœ“ First 10 tokens: {structure_tokens[:10].tolist()}")
        token_names = [structure_loader.id_to_token.get(id.item(), '<UNK>') for id in structure_tokens[:10]]
        print(f"   âœ“ Token names: {token_names}")
        
    except Exception as e:
        print(f"   âœ— Structure loader failed: {e}")
    
    # Test LabelLoader
    print("\n3. Testing LabelLoader...")
    label_loader = LabelLoader()
    
    try:
        label_tokens = label_loader.load_and_process(
            example_data['layout']['data']
        )
        print(f"   âœ“ Label tokens shape: {label_tokens.shape}")
        print(f"   âœ“ Number of label tokens: {len(label_tokens)}")
        print(f"   âœ“ Label vocabulary size: {len(label_loader.token_to_id)}")
        
        # Show first few label tokens
        print(f"   âœ“ First 10 label tokens: {label_tokens[:10].tolist()}")
        label_names = [label_loader.id_to_token.get(id.item(), '<UNK>') for id in label_tokens[:10]]
        print(f"   âœ“ Label token names: {label_names}")
        
    except Exception as e:
        print(f"   âœ— Label loader failed: {e}")

def test_dataset_structure():
    """Test dataset structure and create mock dataset config"""
    print("\n=== Testing Dataset Structure ===\n")
    
    # Create a mock dataset structure for testing
    os.makedirs("mock_dataset/train", exist_ok=True)
    os.makedirs("mock_dataset/val", exist_ok=True)
    os.makedirs("mock_dataset/test", exist_ok=True)
    
    # Move example to mock dataset structure
    os.makedirs("mock_dataset/train/example_0001", exist_ok=True)
    
    # Copy files (simulate having a proper dataset structure)
    import shutil
    if os.path.exists("example-1/example.json"):
        shutil.copy("example-1/example.json", "mock_dataset/train/example_0001/example.json")
    if os.path.exists("example-1/example.png"):
        shutil.copy("example-1/example.png", "mock_dataset/train/example_0001/screenshot.png")
    
    # Update the JSON to have correct screenshot path
    with open("mock_dataset/train/example_0001/example.json", 'r') as f:
        data = json.load(f)
    data['screenshot']['path'] = 'screenshot.png'
    with open("mock_dataset/train/example_0001/example.json", 'w') as f:
        json.dump(data, f, indent=2)
    
    # Create dataset config
    config = {
        'splits': {
            'train': ['example_0001'],
            'val': [],
            'test': []
        }
    }
    
    with open("mock_dataset/dataset_config.yaml", 'w') as f:
        import yaml
        yaml.dump(config, f)
    
    print("âœ“ Mock dataset structure created")
    
    # Test dataset validation
    print("\n4. Testing dataset validation...")
    validation_results = validate_dataset("mock_dataset")
    
    print(f"   âœ“ Valid examples: {len(validation_results['valid_examples'])}")
    print(f"   âœ“ Missing files: {len(validation_results['missing_files'])}")
    print(f"   âœ“ Invalid JSON: {len(validation_results['invalid_json'])}")
    print(f"   âœ“ Vocabulary errors: {len(validation_results['vocab_errors'])}")
    print(f"   âœ“ Syntax errors: {len(validation_results['syntax_errors'])}")
    
    if validation_results['valid_examples']:
        print(f"   âœ“ Valid examples: {validation_results['valid_examples']}")
    
    if validation_results['missing_files']:
        print(f"   âœ— Missing files: {validation_results['missing_files']}")
    
    # Test DatasetLoader
    print("\n5. Testing DatasetLoader...")
    try:
        dataset_loader = DatasetLoader("mock_dataset", split="train")
        print(f"   âœ“ Dataset length: {len(dataset_loader)}")
        
        if len(dataset_loader) > 0:
            # Load first example
            example = dataset_loader[0]
            print(f"   âœ“ Example keys: {list(example.keys())}")
            print(f"   âœ“ Vision patches shape: {example['vision_patches'].shape}")
            print(f"   âœ“ Structure tokens shape: {example['structure_tokens'].shape}")
            print(f"   âœ“ Hierarchy embeddings shape: {example['hierarchy_embeddings'].shape}")
            print(f"   âœ“ Label tokens shape: {example['label_tokens'].shape}")
            print(f"   âœ“ Example ID: {example['example_id']}")
            
    except Exception as e:
        print(f"   âœ— DatasetLoader failed: {e}")

def demonstrate_token_processing():
    """Demonstrate how tokens are processed for complex structures"""
    print("\n=== Demonstrating Token Processing ===\n")
    
    # Load example data
    with open("example-1/example.json", 'r') as f:
        example_data = json.load(f)
    
    print("6. Structure Processing Example:")
    structure_loader = StructureLoader()
    
    # Show a sample of the original structure
    print("   Original structure (first level):")
    for key in list(example_data['structure']['data'].keys())[:1]:
        print(f"     {key}")
    
    structure_tokens, hierarchy_embeddings = structure_loader.load_and_process(
        example_data['structure']['data']
    )
    
    print(f"   Converted to {len(structure_tokens)} tokens with hierarchy info")
    
    print("\n7. Layout Processing Example:")
    label_loader = LabelLoader()
    
    # Show a sample of the layout structure
    print("   Original layout structure:")
    layout_structure = example_data['layout']['data']['structure']
    for key in list(layout_structure.keys())[:1]:
        print(f"     {key}")
        # Show nested structure
        if isinstance(layout_structure[key], dict):
            for nested_key in list(layout_structure[key].keys())[:2]:
                print(f"       â””â”€ {nested_key}")
    
    label_tokens = label_loader.load_and_process(example_data['layout']['data'])
    print(f"   Converted to {len(label_tokens)} label tokens")
    
    # Show how @ concatenation is handled
    print("\n8. @ Concatenation Processing:")
    sample_key = list(layout_structure.keys())[0]
    print(f"   Sample compound key: {sample_key}")
    
    parts = sample_key.split('@')
    print(f"   Split into {len(parts)} parts:")
    for i, part in enumerate(parts):
        print(f"     {i+1}. {part}")

if __name__ == "__main__":
    print("ðŸš€ Testing Data Loader Modules for Section Layout Generation\n")
    
    # Run all tests
    test_individual_loaders()
    test_dataset_structure()
    demonstrate_token_processing()
    
    print("\nâœ… All tests completed successfully!")
    print("\nThe data loaders are ready for use in your transformer model training pipeline.")
    print("Key features implemented:")
    print("- âœ“ Vision patches for ViT-style image processing")
    print("- âœ“ Hierarchical structure token sequences")
    print("- âœ“ Layout target sequences with @ concatenation support")
    print("- âœ“ Comprehensive dataset validation")
    print("- âœ“ Extensible vocabulary management")
