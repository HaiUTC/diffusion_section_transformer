# Multimodal Encoder Configuration - Step 3: Model Architecture
# This configuration defines the architecture specifications from the instruction

# Architecture specifications:
# - Layers: 12
# - Hidden dim: 768
# - Heads: 12

multimodal_encoder:
  # Vision Transformer Branch
  vision_transformer:
    patch_embed_dim: 768 # Dimension of input patch embeddings
    d_model: 768 # Model hidden dimension
    num_layers: 12 # Number of transformer layers
    num_heads: 12 # Number of attention heads
    dropout: 0.1 # Dropout probability
    mask_ratio: 0.5 # MaskDiT masking ratio (50% for 2x speedup)
    max_patches: 1024 # Maximum number of patches supported

  # Structure Transformer Branch
  structure_transformer:
    vocab_size: 1000 # Structure vocabulary size
    token_embed_dim: 768 # Token embedding dimension
    d_model: 768 # Model hidden dimension
    num_layers: 12 # Number of transformer layers
    num_heads: 12 # Number of attention heads
    dropout: 0.1 # Dropout probability
    max_tokens: 512 # Maximum number of tokens supported

  # Token Fusion Module
  token_fusion:
    d_model: 768 # Model hidden dimension
    num_heads: 12 # Number of attention heads
    dropout: 0.1 # Dropout probability
    sparsity_ratio: 0.3 # Sparse fusion pruning ratio (30% reduction)

# Training configuration
training:
  batch_size: 32 # Training batch size
  learning_rate: 1e-4 # Learning rate (AdamW)
  warmup_steps: 10000 # Number of warmup steps
  gradient_clip: 1.0 # Gradient clipping threshold

# Memory optimization
optimization:
  use_gradient_checkpointing: true # Enable gradient checkpointing
  mixed_precision: true # Use mixed precision training
  compile_model: false # Use torch.compile (requires PyTorch 2.0+)

# Hardware configuration
hardware:
  device: "auto" # "auto", "cuda", "mps", or "cpu"
  num_workers: 4 # Number of data loader workers
  pin_memory: true # Pin memory for faster GPU transfer
